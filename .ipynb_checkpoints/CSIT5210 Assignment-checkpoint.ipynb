{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSIT5210 Assignment\n",
    "\n",
    "## Fall 2019\n",
    "\n",
    "### Instructor: Dr. Kenneth Leung (kwtleung@cse.ust.hk)\n",
    "\n",
    "### TA: Dr. Kevin Wang (kevinw@ust.hk)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## [Group Signup link](https://docs.google.com/spreadsheets/d/1pD-LTpKU51jMQ9DymGbQsSfMNJ8JcjGO02sQaXBJzdU/edit#gid=0)\n",
    "\n",
    "## Description\n",
    "\n",
    "In this assignment, you will have an opportunity to apply some data mining techniques that you learned in the class to a problem.\n",
    "\n",
    "To get started on this assignment, you need to download the given dataset and read the description carefully written on this page. Please note that all implementation of your program should be done with Python.\n",
    "\n",
    "You are required to form a team with at most 4 students. You and your groupmates should evenly divide the project tasks yourself. All team members are going to graded with the same score.\n",
    "\n",
    "There are two parts in this assignment. Part 1 is a programming exercise that your groups are required to complete certain tasks using Python. You should complete your Part 1 in this jupyter notebook (.ipynb file) directly. Part 2 is a group presentation that you need to present your findings on the Task 5 of Part 1.\n",
    "\n",
    "## Submission\n",
    "\n",
    "| Part | Due Date | Submission by | Files to Submit |\n",
    "|---|-----|------|----|\n",
    "| Part 1 | 15/11/2019 (Fri) 23:59 | [email to TA](mailto:kevinw@ust.hk) | this `ipynb` file. |\n",
    "| Part 2 | 21/11/2019 (Thu) during lecture | Printed hardcopy | Presentation slides (4 slides per page) |\n",
    "\n",
    "## Prerequesite\n",
    "\n",
    "You are recommended to install the following packages\n",
    "\n",
    "* pandas\n",
    "* geohash\n",
    "* matplotlib\n",
    "* sklearn\n",
    "\n",
    "To install these packages, you shall type in your terminal\n",
    "\n",
    "```\n",
    "> pip install pandas\n",
    "> pip install geohash\n",
    "> pip install matplotlib\n",
    "> pip install sklearn\n",
    "```\n",
    "\n",
    "The package geohash is a little tricky to install.\n",
    "\n",
    "## About the data and the context\n",
    "\n",
    "We are working on some data related to Taxi. In Task 1 to 4 we use the data set `taxi_train.csv`. In Task 5 we use another set of data. \n",
    "You can [download the data here](./tostudent.zip)\n",
    "\n",
    "\n",
    "### Taxi Data\n",
    "\n",
    "The data contains the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>passenger</th>\n",
       "      <th>pickup_geohash</th>\n",
       "      <th>dropoff_geohash</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2009-06-15 17:26:21</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5rxth4yu4c</td>\n",
       "      <td>dr5rxeqnjy8v</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-05 16:52:16</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5reer0t0fm</td>\n",
       "      <td>dr72h81uqhe9</td>\n",
       "      <td>16.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2011-08-18 00:35:00</td>\n",
       "      <td>2</td>\n",
       "      <td>dr5rukr7t72n</td>\n",
       "      <td>dr5ru63jpp74</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2012-04-21 04:30:42</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5rsrhpybjq</td>\n",
       "      <td>dr5ru7bcpe0c</td>\n",
       "      <td>7.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2010-03-09 07:51:00</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5ruvd4f7wb</td>\n",
       "      <td>dr72j06qth30</td>\n",
       "      <td>5.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-06 09:50:45</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5rsnfnuw1v</td>\n",
       "      <td>dr5ruey9ftmf</td>\n",
       "      <td>12.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2012-11-20 20:35:00</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5rud97th2k</td>\n",
       "      <td>dr5rutjbmmr6</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2012-01-04 17:22:00</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5rvnwq9f4p</td>\n",
       "      <td>dr5ru6d01cfq</td>\n",
       "      <td>16.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2012-12-03 13:10:00</td>\n",
       "      <td>1</td>\n",
       "      <td>dr5reyn4k3w6</td>\n",
       "      <td>dr5rsnzyumf1</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2009-09-02 01:11:00</td>\n",
       "      <td>2</td>\n",
       "      <td>dr5rsx2uj8r1</td>\n",
       "      <td>dr5ru7c125sg</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pickup_datetime  passenger pickup_geohash dropoff_geohash  fare\n",
       "0  2009-06-15 17:26:21          1   dr5rxth4yu4c    dr5rxeqnjy8v   4.5\n",
       "1  2010-01-05 16:52:16          1   dr5reer0t0fm    dr72h81uqhe9  16.9\n",
       "2  2011-08-18 00:35:00          2   dr5rukr7t72n    dr5ru63jpp74   5.7\n",
       "3  2012-04-21 04:30:42          1   dr5rsrhpybjq    dr5ru7bcpe0c   7.7\n",
       "4  2010-03-09 07:51:00          1   dr5ruvd4f7wb    dr72j06qth30   5.3\n",
       "5  2011-01-06 09:50:45          1   dr5rsnfnuw1v    dr5ruey9ftmf  12.1\n",
       "6  2012-11-20 20:35:00          1   dr5rud97th2k    dr5rutjbmmr6   7.5\n",
       "7  2012-01-04 17:22:00          1   dr5rvnwq9f4p    dr5ru6d01cfq  16.5\n",
       "8  2012-12-03 13:10:00          1   dr5reyn4k3w6    dr5rsnzyumf1   9.0\n",
       "9  2009-09-02 01:11:00          2   dr5rsx2uj8r1    dr5ru7c125sg   8.9"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "taxi = pd.read_csv('./data/taxi_train.csv', sep=',', nrows= 10)\n",
    "taxi.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `pickup_datetime` is the starting time for the ride.\n",
    "* `passenger` means the number of passenger.\n",
    "* `pickup_geohash` and `dropoff_geohash` are the position where the passenger is picked up and dropped off respectively. They are encoded using geohash.\n",
    "* `fare` is the total amount of paid in the trip.\n",
    "\n",
    "To decode a geoash you might want to use a geohash package (follow hint on Task 1.2 to install it) and it gives you the longitude and latitude of the position. We all know that earth is a sphere (oh please), but for simplicity, when we calculate the distance between two points, we would use Euclidean distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('40.721319', '-73.844311')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geohash as gh\n",
    "gh.decode('dr5rxth4yu4c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Programming Task\n",
    "\n",
    "Your team are required to complete the following tasks on this Jupyter notebook. \n",
    "\n",
    "1. Data Preprocessing & Statistics\n",
    "2. Data Clustering \n",
    "3. Simple Data Visualization \n",
    "4. Frequent Pattern Mining \n",
    "5. Prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 10 columns):\n",
      "pickup_datetime    40000 non-null datetime64[ns]\n",
      "passenger          40000 non-null int64\n",
      "pickup_geohash     40000 non-null object\n",
      "dropoff_geohash    40000 non-null object\n",
      "fare               40000 non-null float64\n",
      "pickup_x           40000 non-null float64\n",
      "pickup_y           40000 non-null float64\n",
      "dropoff_x          40000 non-null float64\n",
      "dropoff_y          40000 non-null float64\n",
      "distance           40000 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(6), int64(1), object(2)\n",
      "memory usage: 3.1+ MB\n",
      "\n",
      "the number of rows removed: 1138\n",
      "Display the Count:\n",
      "1    27025\n",
      "2     5783\n",
      "5     2661\n",
      "3     1687\n",
      "4      803\n",
      "6      766\n",
      "0      137\n",
      "Name: passenger, dtype: int64\n",
      "Display the Mean:\n",
      "1.666538006278627\n",
      "Display the Standard Derviation:\n",
      "1.2868585752227317\n",
      "\n",
      "The earliest pickup_time: 2009-01-01 01:31:49\n",
      "The latest pickup_time: 2015-06-30 22:42:39\n",
      "the number of order between (8am to 9am) and the order between (1am to 2am): 3660\n"
     ]
    }
   ],
   "source": [
    "# Task 1 - Data Preprocessing and Statistics\n",
    "#\n",
    "#\n",
    "# Task 1.1 Read Taxi Data in using the API Pandas.read_csv so that the column 'pickup_datetime' is read as datetime64.\n",
    "#                 Hint: use the parameter parse_dates\n",
    "#                  ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
    "#                 To make sure your code works you might want to read only the first 1000 rows and expand it later\n",
    "import pandas as pd\n",
    "taxi_data = pd.read_csv('./data/taxi_train.csv',parse_dates=['pickup_datetime'])\n",
    "\n",
    "\n",
    "# Task 1.2 Convert the field pickup_geohashed and dropoff_geohashed into x-y coordinate using the API Geohash.decode\n",
    "#                  The package Geohash can be found from pip. You might encounter the problem \n",
    "#                   'python3.5.2 can't find the module '  ref: https://github.com/vinsci/geohash/issues/4\n",
    "#                    This can be fixed very easily. Or\n",
    "#                   You might directly use the fixed version of Geohash in our project package.\n",
    "#                   The precision of each coordinate is with 6 decimal places\n",
    "import geohash as gh\n",
    "pickup_xy = taxi_data['pickup_geohash'].apply(gh.decode)\n",
    "dropoff_xy = taxi_data['dropoff_geohash'].apply(gh.decode)\n",
    "\n",
    "\n",
    "# Task 1.2.1 Create unpack the decoded pickup_geohashed and dropoff_geohashed into the pair of columns \n",
    "#                  pickup_x pickup_y  and dropoff_x, dropoff_y respectively. \n",
    "#                  Namely: if pickup is [40.712278, -73.84161]; pickup_x should contain 40.712278 and pickup_y should contain -73.84161\n",
    "def get_x(t):\n",
    "    return t[0]\n",
    "\n",
    "def get_y(t):\n",
    "    return t[1]\n",
    "\n",
    "# pickup coordinate\n",
    "taxi_data['pickup_x'] = pickup_xy.apply(get_x).apply(float)\n",
    "taxi_data['pickup_y'] = pickup_xy.apply(get_y).apply(float)\n",
    "\n",
    "# dropoff coordinate\n",
    "taxi_data['dropoff_x'] = dropoff_xy.apply(get_x).apply(float)\n",
    "taxi_data['dropoff_y'] = dropoff_xy.apply(get_y).apply(float)\n",
    "\n",
    "\n",
    "# Task 1.3 Create the column 'distance' based on the Euclidean distance that the ride has traveled.\n",
    "import numpy as np\n",
    "def get_distance(t): #Euclidean Distance\n",
    "    dx = t.loc['pickup_x'] - t.loc['dropoff_x']\n",
    "    dy = t.loc['pickup_y'] - t.loc['dropoff_y']\n",
    "    return np.sqrt(dx*dx+dy*dy)\n",
    "taxi_data['distance'] = taxi_data.apply(get_distance,axis=1)\n",
    "\n",
    "\n",
    "# Task 1.4.1  Check the memory you have spent by the API .info()\n",
    "taxi_data.info() #3.1+ MB\n",
    "# Task 1.4.2  Fetch the first 10 lines of your data to preview it.\n",
    "taxi_data.head(10)\n",
    "\n",
    "\n",
    "# Task 1.5 Remove rows with invalid geohashed. Count the number of rows removed.\n",
    "num1 = taxi_data.shape[0]\n",
    "taxi_data = taxi_data[taxi_data['distance']!=0]\n",
    "num2 = taxi_data.shape[0]\n",
    "num_remove = num1-num2  #the number of rows removed\n",
    "print('\\nthe number of rows removed: %d' % (num_remove))\n",
    "\n",
    "\n",
    "# Task 1.6  Display the count, mean, standard derviation of the int type variable and \n",
    "#                  display the earliest and latest pickup_time.\n",
    "data_type = taxi_data.dtypes\n",
    "intlist = data_type[data_type=='int'].index.tolist()\n",
    "print('Display the Count:')\n",
    "print(taxi_data[intlist[0]].value_counts()) #display the count\n",
    "print('Display the Mean:')\n",
    "print(np.mean(taxi_data[intlist[0]]))   #display the mean\n",
    "print('Display the Standard Derviation:')\n",
    "print(np.std(taxi_data[intlist[0]]))    #display the std\n",
    "\n",
    "print('\\nThe earliest pickup_time:', str(min(taxi_data['pickup_datetime']))) #display the earliest pickup_time\n",
    "print('The latest pickup_time:',str(max(taxi_data['pickup_datetime'])))      #display the latest pickup_time\n",
    "\n",
    "\n",
    "# Task 1.7 Find the number of order between (8am to 9am)   and the order between (1am to 2am)\n",
    "#                 Note: Instead of using only the first 1000 rows, expand your selection of rows to collect enough data.\n",
    "#                Hint: try the API between_time of DataFrame. \n",
    "#           ref:  https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.between_time.html#pandas.DataFrame.between_time\n",
    "ptime = taxi_data.loc[:,('pickup_datetime','passenger')].set_index('pickup_datetime') #set time as index\n",
    "num_order = ptime.between_time('08:00','09:00').shape[0] + ptime.between_time('13:00','14:00').shape[0]\n",
    "print('the number of order between (8am to 9am) and the order between (1am to 2am):', num_order)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the number of order has started from a cluster centers and ends at the same cluster centers: 10870\n"
     ]
    }
   ],
   "source": [
    "# Task 2 - Data Clustering \n",
    "#\n",
    "#\n",
    "# Task 2.1 Create a DataFrame that contains two columns. The first column (the index) is a time series 0:00, 0:15, 0:30, 0:45, \n",
    "#                  1:00,... 23:00, 23:15, 23:30, 23:45\n",
    "#                  The second column is an integer that counts the number of ride between in the interval. For example, 0:00 should contains all order happens on or after 0:00 to 0:15.\n",
    "#  This task is less straight forward, at least in our solution. So let's break down a little bit.\n",
    "#  Task 2.1.1 Create a list of string containing the series '0:00', '0:15', '0:30', ... '23:45' \n",
    "#                     Hint: A double loop with if-else can do the job.\n",
    "tlist = [] #time period list\n",
    "for i in range(24):\n",
    "    for j in ['00','15','30','45']:\n",
    "        tstr = str(i)+':'+j\n",
    "        tlist.append(tstr)\n",
    "len_tl = len(tlist)\n",
    "\n",
    "\n",
    "# Task 2.1.2 Count the number of orders. You might use between_time again.\n",
    "norder = np.zeros(len_tl).astype(int) #store the number of orders in time period\n",
    "for i in range(len_tl):\n",
    "    norder[i] = ptime.between_time( tlist[i%len_tl],tlist[(i+1)%len_tl], include_start=True,include_end=False).shape[0]\n",
    "tcluster = pd.DataFrame(columns = ['time_period','order_num']) #create an empty dataframe\n",
    "tcluster['time_period'] = pd.Series(tlist)\n",
    "tcluster['order_num'] = pd.Series(norder)\n",
    "\n",
    "\n",
    "# Task 2.2.1  Use K-mean algorithm to find 30 cluster centers of the coordinates obtained from  Task 1.2.1\n",
    "#                  You may implement your own K-mean algorithm or simply adopt the API sklearn.cluster.KMeans\n",
    "#                   ref: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "from sklearn.cluster import KMeans\n",
    "pickup_data = taxi_data.loc[:,('pickup_x','pickup_y')]      #pickup location\n",
    "dropoff_data = taxi_data.loc[:,('dropoff_x','dropoff_y')]   #dropoff location\n",
    "fitting_data = np.concatenate((pickup_data.values,dropoff_data.values),axis=0) #horizontal concatenation\n",
    "\n",
    "location_kmeans = KMeans(n_clusters=30).fit(fitting_data)  #kmeans model\n",
    "ploc = location_kmeans.labels_[:num2].reshape(num2,1)      #pickup location labels\n",
    "dloc = location_kmeans.labels_[num2:].reshape(num2,1)      #dropoff location labels\n",
    "location_label = np.concatenate((ploc,dloc),axis=1)    #vertical concatenation                   \n",
    "\n",
    "    \n",
    "# Task 2.2.2 Describe how many % of order has started from a cluster centers and ends at the same cluster centers.  \n",
    "num_same_loc = sum(location_label[:,0]==location_label[:,1])\n",
    "print('\\nthe number of order has started from a cluster centers and ends at the same cluster centers:', num_same_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3 - Simple Data Visualization \n",
    "#\n",
    "#\n",
    "# Task 3.1 Using data obtained from Task 1.6. Plot a curveof the volume of order in different times of a day.\n",
    "#                    Hint: try the API DataFrame.plot\n",
    "\n",
    "\n",
    "# Task 3.2 Using data obtained from Task 2.2.1. Plot a histogram of the volume of order in different cluster centers\n",
    "\n",
    "\n",
    "# Task 3.3 Scatter plot the 100 random location of the pickups in blue, 100 random location of dropoffs in red, plot also the cluster centers in black\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4 - Frequent Pattern Mining \n",
    "#\n",
    "#\n",
    "# Task 4.1 Apply FPGrowth algorithm, either using existing API or write your own, to identify which set of users are likely to go-together. \n",
    "#                 Definition of go-together: they starts at the same cluster centers and their start time is in the same 15-minutes timeslot.\n",
    " #                This task may not be straight forward as you may need to build the list of transaction first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5 - Prediction\n",
    "\n",
    "You are given another set of Taxi data while this time there are more columns to play with. With these data you are going to help a Taxi driver, Daniel, to optimize his profit. \n",
    "\n",
    "![](http://file1.telestar.fr/var/telestar/storage/images/3/0/9/9/3099879/samy-naceri-dans-taxi-2_width1024.jpg)  - From the movie *Taxi* - src: telestar.fr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.csv', sep=',', nrows=10)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.1: \n",
    "It is Monday 8am, the prime time for every Taxi Driver. Daniel have received three orders at the same time. Assume Daniel are equally far from these three orders. Please help Daniel to pick the one with largest income (fare + tips). The fields `fare`, `tips` and `best` are masked in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monday8am = pd.read_csv('5_1_testing.csv', sep=',', nrows=10, index_col=0)\n",
    "monday8am.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5.2:\n",
    "It is Daniels holiday and he receives a call from his friend to ask him work as a substitute. His friend will pay him a few hundred to Daniel plus all the tips he earned during the period. Please help Daniel to optimize his profit by picking order. This time, Daniel are given three order at a time. These orders will have the location of the pick up and drop off, and the payment method of the order.  The fields `tips` and `best` are masked in testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tips = pd.read_csv('5_2_testing.csv', sep=',', nrows=10)\n",
    "best_tips.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data given above, predict which order will give you the best result. The accurarcy of your prediction for 5.1 and 5.2 are measured by:\n",
    " * `total_numbers_of_correct_prediction` / `number_of_rows_of_test` * 100%.  \n",
    "An empty prediction would be counted as one wrong prediction. \n",
    "\n",
    "\n",
    "* **A higher accuracy does not immediate imply a higher mark for the project. We value more on how you choose your algorithm , how you fine tune your parameters, and explain why your algorithm did/did not work.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Group Presentation\n",
    "\n",
    "Prepare a 5 minutes presentation on your Task 5 prediction work. You will be given the ground truth after the due day. Compute the accuracy yourself and include it in your presentation. Your presentation should be focus on why would your choose that particular algorithm and what optimization/fine tuning you have done to improve the accuracy. You can also comment on your accuracy and suggest how could that be improved. \n",
    "\n",
    "You may use the following line to measure the accurarcy\n",
    "```\n",
    "groundtruth['predict'] = output['predict']\n",
    "groundtruth[ groundtruth['best'] == groundtruth['predict']].count()\n",
    "```\n",
    "\n",
    "Since this is a short presentation, you may assign any member to present the work.\n",
    "\n",
    "**Note: Again, a higher accuracy does not immediate imply a higher mark.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
